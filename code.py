# -*- coding: utf-8 -*-
"""Untitled137.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LTB02AcWm7i55XrJRrswX5s-oqC6oHai
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Optional, Tuple, Union
from collections import OrderedDict
from sklearn.metrics import roc_auc_score, roc_curve
import math

import flwr
from flwr.client import Client, ClientApp, NumPyClient
from flwr.common import (
    Context,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    Parameters,
    Scalar,
    ndarrays_to_parameters,
    parameters_to_ndarrays,
)
from flwr.server import ServerApp, ServerConfig, ServerAppComponents
from flwr.server.client_manager import ClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.strategy import Strategy
from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg
from flwr.simulation import run_simulation

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Training on {DEVICE}")
print(f"Flower {flwr.__version__} / PyTorch {torch.__version__}")

# ImageBind + Transformer 特征提取器
class ImageBindTransformerExtractor(nn.Module):
    def __init__(self, feature_dim=768, num_frames=16):
        super(ImageBindTransformerExtractor, self).__init__()
        # 设置帧数
        self.num_frames = num_frames

        # 加载预训练的ImageBind模型
        try:
            from imagebind import imagebind_model
            from imagebind.models.imagebind_model import ImageBindModel

            # 初始化ImageBind模型
            self.imagebind = imagebind_model.imagebind_huge(pretrained=True)

            # 冻结ImageBind特征提取器的参数
            for param in self.imagebind.parameters():
                param.requires_grad = False

        except ImportError:
            print("ImageBind不可用，使用ResNet作为备选特征提取器")
            import torchvision.models as models
            resnet = models.resnet50(pretrained=True)
            modules = list(resnet.children())[:-1]  # 移除最后的全连接层
            self.imagebind = nn.Sequential(*modules)
            feature_dim = 2048  # ResNet50的输出维度

        # Transformer编码器模块 - 处理帧序列
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=feature_dim,
                nhead=8,
                dim_feedforward=2048,
                dropout=0.1,
                activation='gelu'
            ),
            num_layers=2
        )

        # 输出映射层
        self.output_layer = nn.Linear(feature_dim, 128)

    def forward(self, x):
        """
        处理16帧视频序列
        x: 输入形状为 [batch_size, num_frames, channels, height, width]
            或 如果是单帧输入 [batch_size, channels, height, width]
        """
        # 检查输入是否为多帧输入
        if len(x.shape) == 5:  # [batch_size, num_frames, channels, height, width]
            batch_size, num_frames, c, h, w = x.shape

            # 提取每帧特征
            frame_features = []

            for frame_idx in range(num_frames):
                # 提取单帧
                frame = x[:, frame_idx]  # [batch_size, channels, height, width]

                try:
                    with torch.no_grad():
                        # 使用ImageBind处理
                        image_inputs = {"vision": frame}
                        embeddings = self.imagebind(image_inputs)
                        frame_feat = embeddings["vision"]  # [batch_size, feature_dim]
                except (AttributeError, TypeError):
                    # 使用备选的ResNet
                    frame_feat = self.imagebind(frame).view(batch_size, -1)

                frame_features.append(frame_feat)

            # 堆叠所有帧特征 [num_frames, batch_size, feature_dim]
            features = torch.stack(frame_features, dim=0)

        else:  # 单帧输入 [batch_size, channels, height, width]
            try:
                with torch.no_grad():
                    image_inputs = {"vision": x}
                    embeddings = self.imagebind(image_inputs)
                    features = embeddings["vision"].unsqueeze(0)  # [1, batch_size, feature_dim]
            except (AttributeError, TypeError):
                batch_size = x.shape[0]
                features = self.imagebind(x).view(batch_size, -1).unsqueeze(0)  # [1, batch_size, feature_dim]

        # 通过Transformer处理序列
        # features形状: [seq_len, batch_size, feature_dim]
        transformed = self.transformer(features)

        # 如果是多帧输入，对所有帧特征进行平均池化
        if len(x.shape) == 5:
            # [seq_len, batch_size, feature_dim] -> [batch_size, feature_dim]
            transformed = transformed.mean(dim=0)
        else:
            # 单帧情况下移除序列维度
            transformed = transformed.squeeze(0)

        # 特征映射
        output = self.output_layer(transformed)

        return output

class TransformerViolenceDetector(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=256, num_layers=4, nhead=8, dropout=0.1):
        super(TransformerViolenceDetector, self).__init__()

        # 位置编码
        self.pos_encoder = PositionalEncoding(input_dim, dropout)

        # Transformer编码器层
        encoder_layers = nn.TransformerEncoderLayer(
            d_model=input_dim,
            nhead=nhead,
            dim_feedforward=hidden_dim,
            dropout=dropout,
            activation='gelu'
        )

        # Transformer编码器
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layers,
            num_layers=num_layers
        )

        # 序列池化 - 将序列表示汇聚为单一向量
        self.attention_pool = nn.Sequential(
            nn.Linear(input_dim, 1),
            nn.Softmax(dim=1)
        )

        # 分类头
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x):
        # 输入形状: [batch_size, seq_len, feature_dim]
        # 转换为Transformer期望的形状: [seq_len, batch_size, feature_dim]
        x = x.permute(1, 0, 2)

        # 添加位置编码
        x = self.pos_encoder(x)

        # 经过Transformer编码器
        x = self.transformer_encoder(x)

        # 转回 [batch_size, seq_len, feature_dim]
        x = x.permute(1, 0, 2)

        # 注意力池化
        attn_weights = self.attention_pool(x)  # [batch_size, seq_len, 1]
        weighted_x = torch.bmm(attn_weights.permute(0, 2, 1), x)  # [batch_size, 1, feature_dim]
        pooled = weighted_x.squeeze(1)  # [batch_size, feature_dim]

        # 分类
        output = self.classifier(pooled)

        return output

# 位置编码模块
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: Tensor, shape [seq_len, batch_size, embedding_dim]
        """
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

# 辅助函数
def get_parameters(net) -> List[np.ndarray]:
    return [val.cpu().numpy() for _, val in net.state_dict().items()]

def set_parameters(net, parameters: List[np.ndarray]):
    params_dict = zip(net.state_dict().keys(), parameters)
    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
    net.load_state_dict(state_dict, strict=True)

# 视频序列数据集类
class VideoSequenceDataset(Dataset):
    def __init__(self, video_sequences, labels):
        self.video_sequences = video_sequences
        self.labels = labels

    def __len__(self):
        return len(self.video_sequences)

    def __getitem__(self, idx):
        return self.video_sequences[idx], self.labels[idx]

# 第一阶段：特征提取和伪标签生成函数
def extract_features(video, feature_extractor):
    """从视频序列中提取特征"""
    features = []
    for frame in video:
        frame_tensor = frame.to(DEVICE)
        with torch.no_grad():
            feature = feature_extractor(frame_tensor.unsqueeze(0)).squeeze(0)
        features.append(feature.cpu().numpy())
    return np.array(features)  # 返回形状为 [seq_len, feature_dim] 的特征序列

def compute_statistical_representation(features):
    """计算统计表示（算法第7-12行）"""
    m_i = len(features)
    if m_i <= 2:
        return [0.0, 0.0]  # 处理边缘情况

    # 计算范数
    norms = [np.linalg.norm(feat, ord=2) for feat in features]

    # 范数差异均值（第8行）
    norm_diffs = [abs(norms[j] - norms[j+1]) for j in range(m_i-1)]
    mu_i = np.mean(norm_diffs)

    # 范数差异标准差（第9行）
    sigma_i = np.std(norm_diffs, ddof=1)

    # 平均特征范数（第10行）
    f_bar_i = np.mean(norms)

    # 范数变异性（第11行）
    G_i = 0
    for j in range(m_i):
        for k in range(m_i):
            G_i += abs(norms[j] - norms[k])
    G_i /= (2 * (m_i**2) * f_bar_i) if f_bar_i > 0 else 1

    return [sigma_i, G_i]

def coarse_grained_pseudo_label_assignment(statistical_representations, percentile=80):
    """粗粒度伪标签分配（算法第13-18行）"""
    # 计算本地中心
    center = np.mean(statistical_representations, axis=0)

    # 计算距离
    distances = [np.linalg.norm(x - center) for x in statistical_representations]

    # 将半径设置为p百分位
    radius = np.percentile(distances, percentile)

    # 分配伪标签
    pseudo_labels = [1 if d > radius else 0 for d in distances]

    return pseudo_labels

def fine_grained_pseudo_label_assignment(anomalous_indices, all_features, pca_matrix, normal_features, window_length=5, anomaly_threshold=3.0):
    """为异常视频片段分配细粒度伪标签（算法第19-33行）"""
    fine_grained_labels = {}

    # 压平并投影正常特征
    all_normal_segments = []
    for video_feats in normal_features:
        all_normal_segments.extend(video_feats)

    projected_normal = [np.dot(pca_matrix.T, feat) for feat in all_normal_segments]

    # 计算均值和协方差（第22-24行）
    mu = np.mean(projected_normal, axis=0)
    sigma_diag = np.diag(np.var(projected_normal, axis=0))
    sigma_inv = np.linalg.inv(sigma_diag)

    for idx in anomalous_indices:
        video_features = all_features[idx]
        m_i = len(video_features)

        # 投影特征（第21行）
        projected_features = [np.dot(pca_matrix.T, feat) for feat in video_features]

        # 计算马氏距离（第27行）
        distances = []
        for z_ij in projected_features:
            diff = z_ij - mu
            d_ij = np.sqrt(np.dot(np.dot(diff, sigma_inv), diff))
            distances.append(d_ij)

        # 计算累积和（第28行）
        cum_sum = np.cumsum([0] + distances)

        # 查找具有最大平均值的窗口（第29-31行）
        max_avg = 0
        max_t = 0

        for t in range(1, m_i - window_length + 2):
            window_avg = (cum_sum[t + window_length - 1] - cum_sum[t - 1]) / window_length
            if window_avg > max_avg:
                max_avg = window_avg
                max_t = t - 1

        # 分配细粒度标签（第32-33行）
        segment_labels = []
        for j in range(m_i):
            if j in range(max_t, max_t + window_length) or distances[j] > anomaly_threshold:
                segment_labels.append(0)  # 非暴力
            else:
                segment_labels.append(1)  # 暴力

        fine_grained_labels[idx] = segment_labels

    return fine_grained_labels

# IFLVADDSA客户端
class IFLVADDSAClient(NumPyClient):
    def __init__(self, partition_id, videos, feature_extractor, model, pca_matrix,
                 window_length=5, anomaly_threshold=3.0, percentile=80):
        self.partition_id = partition_id
        self.videos = videos
        self.feature_extractor = feature_extractor
        self.model = model
        self.pca_matrix = pca_matrix
        self.window_length = window_length
        self.anomaly_threshold = anomaly_threshold
        self.percentile = percentile
        self.current_round = 0  # 添加轮次跟踪

        # 生成伪标签（第I阶段）
        self.features, self.pseudo_labels, self.statistical_info = self.generate_pseudo_labels()

        # 创建数据集和数据加载器
        self.trainloader, self.valloader = self.create_data_loaders()

    def generate_pseudo_labels(self):
        """实现第I阶段：本地伪标签生成"""
        print(f"[客户端 {self.partition_id}] 生成伪标签")

        # 为所有视频提取特征（第3-7行）
        all_features = [extract_features(video, self.feature_extractor) for video in self.videos]

        # 计算统计表示（第7-12行）
        statistical_reps = [compute_statistical_representation(features) for features in all_features]

        # 粗粒度伪标签分配（第13-18行）
        coarse_labels = coarse_grained_pseudo_label_assignment(statistical_reps, self.percentile)

        # 识别正常和异常视频
        normal_indices = [i for i, label in enumerate(coarse_labels) if label == 0]
        anomalous_indices = [i for i, label in enumerate(coarse_labels) if label == 1]

        normal_features = [all_features[i] for i in normal_indices]

        # 为异常视频分配细粒度伪标签（第19-33行）
        fine_grained_labels = {}
        if normal_features:  # 只有当我们有正常视频作为参考时
            fine_grained_labels = fine_grained_pseudo_label_assignment(
                anomalous_indices, all_features, self.pca_matrix, normal_features,
                self.window_length, self.anomaly_threshold
            )

        # 结合粗粒度和细粒度标签（第34-35行）
        pseudo_labels = {}
        for i, label in enumerate(coarse_labels):
            if i in anomalous_indices and i in fine_grained_labels:
                pseudo_labels[i] = (label, fine_grained_labels[i])
            else:
                pseudo_labels[i] = (label, None)

        # 计算辅助统计信息（第36-38行）
        avg_sigma = np.mean([sr[0] for sr in statistical_reps])
        avg_g = np.mean([sr[1] for sr in statistical_reps])

        return all_features, pseudo_labels, (avg_sigma, avg_g)

    def create_data_loaders(self):
        """根据伪标签创建数据加载器"""
        train_sequences = []
        train_labels = []

        # 根据伪标签创建数据集
        for i, video in enumerate(self.videos):
            # 提取特征
            feature_sequence = torch.tensor(self.features[i], dtype=torch.float32)
            coarse_label, fine_labels = self.pseudo_labels[i]

            if fine_labels is None:
                # 对于正常视频，使用视频级标签
                train_sequences.append(feature_sequence)
                train_labels.append(float(coarse_label))
            else:
                # 对于异常视频，使用片段级标签
                # 创建滑动窗口，每个窗口分配一个标签
                for j in range(len(fine_labels) - self.window_length + 1):
                    window_features = feature_sequence[j:j+self.window_length]
                    window_label = 1 if sum(fine_labels[j:j+self.window_length]) > 0 else 0
                    train_sequences.append(window_features)
                    train_labels.append(float(window_label))

        # 创建数据集并分为训练/验证
        dataset = VideoSequenceDataset(train_sequences, train_labels)
        dataset_size = len(dataset)
        train_size = int(0.8 * dataset_size)
        val_size = dataset_size - train_size

        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )

        # 创建数据加载器
        trainloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
        valloader = DataLoader(val_dataset, batch_size=16)

        return trainloader, valloader

    def get_parameters(self, config):
        """获取模型参数"""
        print(f"[客户端 {self.partition_id}] get_parameters")
        return get_parameters(self.model)

    def fit(self, parameters, config):
        """实现第II阶段：本地模型训练和更新（第42-46行）"""
        print(f"[客户端 {self.partition_id}] fit, config: {config}")

        # 从配置获取轮次信息
        self.current_round = config.get("server_round", 1)

        # 更新阈值（如果服务器提供）
        if "anomaly_threshold" in config:
            self.anomaly_threshold = config["anomaly_threshold"]
            print(f"[客户端 {self.partition_id}] 更新异常阈值为: {self.anomaly_threshold}")

        # 设置模型参数
        set_parameters(self.model, parameters)

        # 从配置中获取学习率
        lr = config.get("lr", 0.001)
        epochs = config.get("epochs", 1)

        # 使用二元交叉熵损失训练模型（第44行）
        criterion = torch.nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)

        self.model.train()
        for epoch in range(epochs):
            total_loss = 0.0
            for sequences, labels in self.trainloader:
                sequences, labels = sequences.to(DEVICE), labels.float().to(DEVICE).view(-1, 1)

                optimizer.zero_grad()
                outputs = self.model(sequences)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            print(f"[客户端 {self.partition_id}] Epoch {epoch+1}: loss {total_loss/len(self.trainloader)}")

        # 计算本地更新（第45行）
        updated_parameters = get_parameters(self.model)

        # 评估模型质量（第46行）- 使用AUC
        self.model.eval()
        all_labels = []
        all_preds = []
        with torch.no_grad():
            for sequences, labels in self.valloader:
                sequences, labels = sequences.to(DEVICE), labels.float().to(DEVICE)
                outputs = self.model(sequences)
                probs = torch.sigmoid(outputs).cpu().numpy().flatten()
                all_preds.extend(probs)
                all_labels.extend(labels.cpu().numpy().flatten())

        auc = roc_auc_score(all_labels, all_preds) if len(set(all_labels)) > 1 else 0.5
        print(f"[客户端 {self.partition_id}] Model AUC: {auc}")

        # 如果质量低或服务器强制更新，更新伪标签
        label_update_stats = None
        if auc < 0.6 or config.get("force_label_update", False):
            label_update_stats = self.update_pseudo_labels()

        # 返回参数、示例数和指标（第47行）
        return updated_parameters, len(self.trainloader.dataset), {
            "auc": auc,
            "avg_sigma": self.statistical_info[0],
            "avg_g": self.statistical_info[1],
            "label_updates": label_update_stats
        }

    def update_pseudo_labels(self):
        """基于模型预测更新伪标签 - 增强版"""
        self.model.eval()

        # 保存原始伪标签以便比较
        original_pseudo_labels = self.pseudo_labels.copy()

        # 为所有视频重新生成预测和伪标签
        new_pseudo_labels = {}
        update_statistics = {"changed_video_labels": 0, "changed_segment_labels": 0}

        for i, video in enumerate(self.videos):
            # 提取特征序列
            feature_sequence = torch.tensor(self.features[i], dtype=torch.float32).to(DEVICE)

            # 提取原始标签
            orig_video_label, orig_segment_labels = original_pseudo_labels.get(i, (0, None))

            # 应用模型进行预测
            segment_probs = []
            segment_preds = []
            for j in range(len(feature_sequence) - self.window_length + 1):
                window = feature_sequence[j:j+self.window_length].unsqueeze(0)
                with torch.no_grad():
                    output = self.model(window)
                    prob = torch.sigmoid(output).item()
                    pred = 1 if prob > 0.5 else 0
                    segment_probs.append(prob)
                    segment_preds.append(pred)

            # 确定视频级标签 - 使用概率平均值而不仅是二进制预测
            avg_prob = sum(segment_probs) / len(segment_probs) if segment_probs else 0.5
            model_video_label = 1 if avg_prob > 0.5 else 0

            # 融合原始标签和模型预测 - 可信度加权
            confidence_weight = min(0.2 * self.current_round, 0.8)  # 随训练进行增加模型权重
            final_video_label = round(model_video_label * confidence_weight +
                                     orig_video_label * (1 - confidence_weight))

            # 如果预测为暴力，分配段级标签
            if final_video_label == 1:
                # 为视频的每一段分配标签
                full_segment_labels = []

                # 处理所有段
                for j in range(len(feature_sequence)):
                    # 找到这个段所在的所有窗口
                    window_predictions = []
                    window_probs = []
                    for w in range(max(0, j - self.window_length + 1), min(j + 1, len(segment_preds))):
                        window_predictions.append(segment_preds[w])
                        window_probs.append(segment_probs[w])

                    # 如果没有窗口包含该段，使用最近的窗口
                    if not window_predictions:
                        nearest_window = min(max(0, j - self.window_length + 1),
                                            len(segment_preds) - 1 if segment_preds else 0)
                        pred = segment_preds[nearest_window] if segment_preds else 0
                        orig_pred = orig_segment_labels[j] if orig_segment_labels and j < len(orig_segment_labels) else 0
                        # 融合原始标签和模型预测
                        final_pred = round(pred * confidence_weight + orig_pred * (1 - confidence_weight))
                    else:
                        # 计算该段在所有窗口中的加权预测
                        weighted_pred = sum(window_probs) / len(window_probs) if window_probs else 0.5
                        model_pred = 1 if weighted_pred > 0.5 else 0

                        # 获取原始标签
                        orig_pred = orig_segment_labels[j] if orig_segment_labels and j < len(orig_segment_labels) else 0

                        # 融合原始标签和模型预测
                        final_pred = round(model_pred * confidence_weight + orig_pred * (1 - confidence_weight))

                    full_segment_labels.append(final_pred)

                # 统计标签变化
                if orig_video_label != final_video_label:
                    update_statistics["changed_video_labels"] += 1

                if orig_segment_labels:
                    changes = sum(1 for a, b in zip(orig_segment_labels, full_segment_labels) if a != b)
                    update_statistics["changed_segment_labels"] += changes

                new_pseudo_labels[i] = (final_video_label, full_segment_labels)
            else:
                if orig_video_label != final_video_label:
                    update_statistics["changed_video_labels"] += 1

                new_pseudo_labels[i] = (final_video_label, None)

        # 更新伪标签
        self.pseudo_labels = new_pseudo_labels

        # 重新创建数据加载器
        self.trainloader, self.valloader = self.create_data_loaders()

        print(f"[客户端 {self.partition_id}] 标签更新统计: 变更视频标签: {update_statistics['changed_video_labels']}, "
              f"变更片段标签: {update_statistics['changed_segment_labels']}")

        # 返回标签更新统计以便服务器端使用
        return update_statistics

    def evaluate(self, parameters, config):
        """评估模型，使用AUC作为指标"""
        print(f"[客户端 {self.partition_id}] evaluate, config: {config}")

        # 如果提供了更新的PCA矩阵，则更新
        if "pca_matrix" in config:
            self.pca_matrix = np.array(config["pca_matrix"])

        # 如果提供了更新的异常阈值，则更新
        if "anomaly_threshold" in config:
            self.anomaly_threshold = config["anomaly_threshold"]

        # 设置模型参数
        set_parameters(self.model, parameters)

        # 评估模型
        criterion = torch.nn.BCEWithLogitsLoss()
        self.model.eval()
        loss = 0.0
        all_labels = []
        all_preds = []

        with torch.no_grad():
            for sequences, labels in self.valloader:
                sequences, labels = sequences.to(DEVICE), labels.float().to(DEVICE).view(-1, 1)
                outputs = self.model(sequences)
                loss += criterion(outputs, labels).item() * labels.size(0)

                probs = torch.sigmoid(outputs).cpu().numpy().flatten()
                all_preds.extend(probs)
                all_labels.extend(labels.cpu().numpy().flatten())

        # 计算AUC
        auc = roc_auc_score(all_labels, all_preds) if len(set(all_labels)) > 1 else 0.5
        avg_loss = loss / len(self.valloader.dataset) if len(self.valloader.dataset) > 0 else 0

        return avg_loss, len(self.valloader.dataset), {"auc": auc}

# IFLVADDSA策略
class IFLVADDSAStrategy(Strategy):
    def __init__(
        self,
        fraction_fit: float = 1.0,
        fraction_evaluate: float = 1.0,
        min_fit_clients: int = 2,
        min_evaluate_clients: int = 2,
        min_available_clients: int = 2,
        alpha: float = 0.7,
        beta: float = 0.3,
        global_lr: float = 0.1
    ) -> None:
        super().__init__()
        self.fraction_fit = fraction_fit
        self.fraction_evaluate = fraction_evaluate
        self.min_fit_clients = min_fit_clients
        self.min_evaluate_clients = min_evaluate_clients
        self.min_available_clients = min_available_clients
        self.alpha = alpha  # Shapley权重参数
        self.beta = beta    # Shapley权重参数
        self.global_lr = global_lr
        self.global_statistical_model = None
        self.pca_matrix = None
        self.anomaly_threshold = 3.0  # 初始异常阈值
        self.reduced_dim = 64  # 初始降维维度
        self.clients_per_round = 0  # 每轮参与的客户端数量
        self.force_label_update = False  # 是否强制客户端更新标签

    def __repr__(self) -> str:
        return "IFLVADDSAStrategy"

    def initialize_parameters(
        self, client_manager: ClientManager
    ) -> Optional[Parameters]:
        """初始化全局模型参数"""
        model = TransformerViolenceDetector()
        return ndarrays_to_parameters(get_parameters(model))

    def configure_fit(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, FitIns]]:
        """配置下一轮训练"""
        # 采样客户端
        sample_size, min_num_clients = self.num_fit_clients(
            client_manager.num_available()
        )
        clients = client_manager.sample(
            num_clients=sample_size, min_num_clients=min_num_clients
        )

        # 创建配置
        fit_configurations = []
        for client in clients:
            config = {
                "lr": 0.001,
                "epochs": 1,
                "server_round": server_round,
                "force_label_update": getattr(self, "force_label_update", False),
                "anomaly_threshold": self.anomaly_threshold
            }

            # 如果可用，发送PCA矩阵
            if self.pca_matrix is not None:
                config["pca_matrix"] = self.pca_matrix.tolist()

            fit_configurations.append((client, FitIns(parameters, config)))

        return fit_configurations

    def aggregate_fit(
        self,
        server_round: int,
        results: List[Tuple[ClientProxy, FitRes]],
        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],
    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:
        """实现第III阶段：使用解耦Shapley贡献的全局聚合（第48-56行）"""
        if not results:
            return None, {}

        # 记录本轮参与的客户端数量，用于伪标签规则更新
        self.clients_per_round = len(results)

        # 构建全局统计模型（第49行）
        self._update_global_statistical_model(results)

        # 计算基于Shapley的权重（第50-55行）
        client_weights = self._compute_shapley_weights(results)

        # 获取当前全局模型参数
        current_global_parameters = parameters_to_ndarrays(self.parameters)

        # 计算每个客户端的更新量（delta）并进行加权聚合
        deltas = []
        for (_, fit_res), weight in zip(results, client_weights):
            client_parameters = parameters_to_ndarrays(fit_res.parameters)
            # 计算更新量：客户端参数 - 全局参数
            delta = [(client_param - global_param) for client_param, global_param
                    in zip(client_parameters, current_global_parameters)]
            deltas.append((delta, weight))

        # 应用Shapley加权聚合和全局学习率（第56行）
        updated_parameters = self._aggregate_with_lr(deltas, current_global_parameters)
        parameters_aggregated = ndarrays_to_parameters(updated_parameters)

        # 更新PCA矩阵和伪标签生成规则（第57行）
        updated_pca = False
        updated_rules = False

        if server_round % 5 == 0:  # 每5轮更新一次PCA矩阵
            self._update_pca_matrix()
            updated_pca = True

        # 更新伪标签生成规则
        updated_rules = self.update_pseudolabel_rules(results)

        # 如果规则更新了，在下一轮强制客户端更新标签
        self.force_label_update = updated_rules or server_round % 10 == 0

        # 计算指标
        metrics = {
            "round": server_round,
            "updated_pca": updated_pca,
            "updated_rules": updated_rules,
            "force_label_update": self.force_label_update,
            "anomaly_threshold": self.anomaly_threshold
        }

        return parameters_aggregated, metrics

    def update_pseudolabel_rules(self, results):
        """基于客户端的标签更新统计更新伪标签生成规则"""
        total_updates = {"videos": 0, "segments": 0}
        clients_with_updates = 0

        for _, fit_res in results:
            metrics = fit_res.metrics
            if "label_updates" in metrics and metrics["label_updates"]:
                updates = metrics["label_updates"]
                total_updates["videos"] += updates.get("changed_video_labels", 0)
                total_updates["segments"] += updates.get("changed_segment_labels", 0)
                clients_with_updates += 1

        if clients_with_updates > 0:
            # 基于标签更新情况调整阈值
            # 如果大量标签被更新，可能需要调整伪标签生成的阈值
            avg_video_updates = total_updates["videos"] / clients_with_updates
            avg_segment_updates = total_updates["segments"] / clients_with_updates

            print(f"平均每个客户端更新 {avg_video_updates:.2f} 个视频标签和 {avg_segment_updates:.2f} 个片段标签")

            # 动态调整PCA矩阵维度 - 例如，如果标签变化大，增加维度以捕获更多变异
            if avg_segment_updates > 10:  # 假设阈值
                self.reduced_dim = min(128, self.reduced_dim + 4)  # 增加维度
                print(f"增加PCA降维维度到 {self.reduced_dim}")
            elif avg_segment_updates < 2:  # 较少更新
                self.reduced_dim = max(32, self.reduced_dim - 2)  # 减少维度
                print(f"减少PCA降维维度到 {self.reduced_dim}")

            # 调整伪标签生成的阈值
            if avg_video_updates > 0.3 * self.clients_per_round:  # 如果大部分客户端更新了标签
                self.anomaly_threshold *= 0.9  # 降低异常阈值
                print(f"降低异常阈值到 {self.anomaly_threshold}")
            elif avg_video_updates < 0.1 * self.clients_per_round:
                self.anomaly_threshold *= 1.1  # 提高异常阈值
                print(f"提高异常阈值到 {self.anomaly_threshold}")

            return True  # 指示规则已更新

        return False  # 没有更新规则

    def _update_global_statistical_model(self, results):
        """使用客户端信息更新全局统计模型"""
        client_stats = []
        for _, fit_res in results:
            metrics = fit_res.metrics
            if "avg_sigma" in metrics and "avg_g" in metrics:
                client_stats.append([metrics["avg_sigma"], metrics["avg_g"]])

        if client_stats:
            self.global_statistical_model = np.mean(client_stats, axis=0)

    def _compute_shapley_weights(self, results):
        """计算客户端的Shapley权重"""
        if self.global_statistical_model is None:
            # 如果还没有全局模型，使用相等的权重
            return [1.0 for _ in results]

        n_clients = len(results)
        weights = []

        # 对于每个客户端，计算个体重要性（第52行）
        for i, (_, fit_res) in enumerate(results):
            # 使用AUC而不是准确率作为质量指标
            auc = fit_res.metrics.get("auc", 0.5)
            individual_importance = auc - 0.5  # 基准是随机猜测

            # 计算与其他客户端的交互重要性（第53-54行）
            interaction_importance = 0
            for j, (_, other_res) in enumerate(results):
                if i != j:
                    other_auc = other_res.metrics.get("auc", 0.5)
                    # 简化的交互计算
                    interaction = ((auc + other_auc) / 2) - 0.5
                    interaction_importance += interaction

            # 结合alpha和beta权重（第55行）
            weight = self.alpha * individual_importance + self.beta * interaction_importance

            # 确保权重为正
            weights.append(max(0.1, weight))

        return weights

    def _aggregate_with_lr(self, deltas, current_global_parameters):
        """按照Shapley值聚合客户端更新并应用到全局模型"""
        # 聚合客户端更新（按Shapley值加权）
        aggregated_delta = []
        for i in range(len(current_global_parameters)):
            weighted_sum_delta = np.zeros_like(current_global_parameters[i])
            weight_sum = 0.0

            for delta_params, weight in deltas:
                weighted_sum_delta += delta_params[i] * weight
                weight_sum += weight

            # 应用全局学习率并加到当前全局参数
            if weight_sum > 0:
                aggregated_delta.append(
                    current_global_parameters[i] + (weighted_sum_delta / weight_sum) * self.global_lr
                )
            else:
                aggregated_delta.append(current_global_parameters[i])

        return aggregated_delta

    def _update_pca_matrix(self):
        """更新PCA矩阵"""

        feature_dim = 128  # 特征维度
        reduced_dim = self.reduced_dim  # 使用动态调整的降维维度

        # 创建随机矩阵并正交化
        matrix = np.random.randn(feature_dim, reduced_dim)
        q, r = np.linalg.qr(matrix)
        self.pca_matrix = q
        print(f"已更新PCA矩阵，降维维度: {reduced_dim}")

    def configure_evaluate(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, EvaluateIns]]:
        """配置下一轮评估"""
        if self.fraction_evaluate == 0.0:
            return []

        # 采样客户端
        sample_size, min_num_clients = self.num_evaluation_clients(
            client_manager.num_available()
        )
        clients = client_manager.sample(
            num_clients=sample_size, min_num_clients=min_num_clients
        )

        # 创建评估指令
        config = {
            "round": server_round,
            "anomaly_threshold": self.anomaly_threshold
        }
        if self.pca_matrix is not None:
            config["pca_matrix"] = self.pca_matrix.tolist()

        return [(client, EvaluateIns(parameters, config)) for client in clients]

    def aggregate_evaluate(
        self,
        server_round: int,
        results: List[Tuple[ClientProxy, EvaluateRes]],
        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],
    ) -> Tuple[Optional[float], Dict[str, Scalar]]:
        """聚合评估结果"""
        if not results:
            return None, {}


        loss_aggregated = weighted_loss_avg(
            [(evaluate_res.num_examples, evaluate_res.loss) for _, evaluate_res in results]
        )

        # 计算平均AUC
        aucs = [res.metrics["auc"] for _, res in results if "auc" in res.metrics]
        avg_auc = sum(aucs) / len(aucs) if aucs else 0.5

        return loss_aggregated, {"auc": avg_auc}

    def evaluate(
        self, server_round: int, parameters: Parameters
    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:
        """评估全局模型参数"""
        # 该策略中没有中央评估
        return None

    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:
        """返回样本大小和所需的客户端数量"""
        num_clients = int(num_available_clients * self.fraction_fit)
        return max(num_clients, self.min_fit_clients), self.min_available_clients

    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:
        """使用一部分可用客户端进行评估"""
        num_clients = int(num_available_clients * self.fraction_evaluate)
        return max(num_clients, self.min_evaluate_clients), self.min_available_clients

# 模拟视频数据加载函数
def load_video_data(partition_id, num_partitions):
    """从XD-Violence数据集加载视频数据"""
    import os
    import cv2
    import json
    import random
    import torchvision.transforms as transforms

    # 设置随机种子以确保可复现性
    random.seed(partition_id)
    np.random.seed(partition_id)

    # XD-Violence数据集路径
    dataset_path = "/content/XD-Violence"
    video_dir = os.path.join(dataset_path, "videos")
    annotation_file = os.path.join(dataset_path, "annotations.json")

    # 检查路径是否存在
    if not os.path.exists(video_dir):
        raise FileNotFoundError(f"视频目录不存在: {video_dir}")
    if not os.path.exists(annotation_file):
        raise FileNotFoundError(f"标注文件不存在: {annotation_file}")

    # 加载标注文件
    with open(annotation_file, 'r') as f:
        annotations = json.load(f)

    # 获取视频文件列表
    video_files = []
    for file in os.listdir(video_dir):
        if file.endswith(('.mp4', '.avi', '.mov')):
            video_name = os.path.splitext(file)[0]
            if video_name in annotations:
                video_files.append(file)

    # 如果视频文件不足，尝试其他可能的视频格式
    if len(video_files) == 0:
        print("未找到标准格式的视频文件，尝试查找所有可能的视频文件")
        video_files = [f for f in os.listdir(video_dir) if os.path.isfile(os.path.join(video_dir, f))]
        print(f"找到的文件: {video_files[:5]}...")

    # 确保我们有足够的视频
    if len(video_files) == 0:
        raise ValueError(f"在 {video_dir} 中未找到视频文件")

    print(f"共找到 {len(video_files)} 个视频文件")

    # 为当前客户端分配视频
    total_videos = len(video_files)
    videos_per_client = max(1, total_videos // num_partitions)

    start_idx = partition_id * videos_per_client
    end_idx = min((partition_id + 1) * videos_per_client, total_videos)

    # 如果是最后一个客户端，确保包含所有剩余视频
    if partition_id == num_partitions - 1:
        end_idx = total_videos

    client_video_files = video_files[start_idx:end_idx]

    # 为了测试性能，限制每个客户端的视频数量
    max_videos_per_client = 10
    if len(client_video_files) > max_videos_per_client:
        client_video_files = random.sample(client_video_files, max_videos_per_client)

    print(f"客户端 {partition_id} 分配了 {len(client_video_files)} 个视频")

    # 定义图像转换
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),  # 调整为标准输入尺寸
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet标准化
    ])

    # 加载分配给此客户端的视频
    videos = []
    successful_videos = 0

    for video_file in client_video_files:
        video_path = os.path.join(video_dir, video_file)

        try:
            # 打开视频文件
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"无法打开视频: {video_path}")
                continue

            # 获取视频属性
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            # 为大型视频选择关键帧，处理16帧/段
            frames_per_segment = 16

            # 计算采样间隔，确保至少获取4个段
            min_segments = 4
            max_segments = 10

            # 计算每个段的起始帧
            if frame_count <= frames_per_segment * min_segments:
                # 视频较短，采样所有帧
                segment_starts = list(range(0, frame_count, frames_per_segment))
            else:
                # 视频较长，均匀采样
                num_segments = min(max_segments, frame_count // frames_per_segment)
                segment_stride = frame_count // num_segments
                segment_starts = [i * segment_stride for i in range(num_segments)]

            # 读取每个段的帧
            video_segments = []

            for start_frame in segment_starts:
                # 设置读取位置
                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

                # 读取一个段的帧
                segment_frames = []
                for _ in range(frames_per_segment):
                    ret, frame = cap.read()
                    if not ret:
                        break  # 视频结束

                    # 转换BGR到RGB
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                    # 应用转换
                    frame_tensor = transform(frame)
                    segment_frames.append(frame_tensor)

                # 如果段包含足够帧，则添加到段列表
                if len(segment_frames) == frames_per_segment:
                    # 将段的所有帧堆叠为一个张量 [frames_per_segment, channels, height, width]
                    segment_tensor = torch.stack(segment_frames, dim=0)
                    video_segments.append(segment_tensor)

            # 释放视频资源
            cap.release()

            # 如果至少提取了一个完整段，则添加到视频列表
            if video_segments:
                videos.append(video_segments)
                successful_videos += 1
                print(f"成功加载视频 {video_file}，提取了 {len(video_segments)} 个段")
            else:
                print(f"视频 {video_file} 无法提取完整段")

        except Exception as e:
            print(f"处理视频 {video_file} 时出错: {str(e)}")

    # 如果没有成功加载任何视频，抛出异常
    if successful_videos == 0:
        raise ValueError(f"客户端 {partition_id} 未能成功加载任何视频，请检查数据集路径和视频格式")

    print(f"客户端 {partition_id} 成功加载 {successful_videos} 个视频")

    return videos
# Flower客户端函数
def client_fn(context: Context) -> Client:
    """创建Flower客户端"""
    import math
    # 提取客户端信息
    partition_id = context.node_config["partition-id"]
    num_partitions = context.node_config["num-partitions"]

    # 创建模型
    feature_extractor = ImageBindTransformerExtractor().to(DEVICE)
    model = TransformerViolenceDetector(input_dim=128, hidden_dim=256).to(DEVICE)

    # 加载视频数据
    videos = load_video_data(partition_id, num_partitions)

    # 初始PCA矩阵（将由服务器更新）
    feature_dim = 128
    reduced_dim = 64
    pca_matrix = np.eye(feature_dim, reduced_dim)

    # 创建客户端
    client = IFLVADDSAClient(
        partition_id=partition_id,
        videos=videos,
        feature_extractor=feature_extractor,
        model=model,
        pca_matrix=pca_matrix,
        window_length=5,
        anomaly_threshold=3.0,
        percentile=80
    )

    return client.to_client()

# 主函数运行模拟
def main():
    import math
    # 客户端数量
    NUM_PARTITIONS = 5

    # 创建客户端应用
    client_app = ClientApp(client_fn=client_fn)

    # 创建带有自定义策略的服务器
    def server_fn(context: Context) -> ServerAppComponents:
        config = ServerConfig(num_rounds=10)
        strategy = IFLVADDSAStrategy(
            fraction_fit=0.8,
            fraction_evaluate=0.5,
            min_fit_clients=2,
            min_evaluate_clients=2,
            min_available_clients=2,
            alpha=0.7,
            beta=0.3,
            global_lr=0.1
        )
        return ServerAppComponents(config=config, strategy=strategy)

    server_app = ServerApp(server_fn=server_fn)

    # 配置资源
    backend_config = {"client_resources": None}
    if torch.cuda.is_available():
        backend_config = {"client_resources": {"num_gpus": 1}}

    # 运行模拟
    run_simulation(
        server_app=server_app,
        client_app=client_app,
        num_supernodes=NUM_PARTITIONS,
        backend_config=backend_config,
    )

if __name__ == "__main__":
    import math
    main()